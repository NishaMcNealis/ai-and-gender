
---
layout: post
title:  Solutions
permalink: /solutions/
---

There are several measures we can take to mitigate the negative effects of predominantly female VAs.
First and foremost, as consumers and employees we should pressure creators of these technologies to reduce bias. Big tech companies are gradually becoming more transparent with their data and more upfront about any potential biases. Google’s DeepMind, for instance, is working on a new text-to-speech algorithm that requires less data, which would increase accuracy for male voice assistants. Google also now randomly assigns new users to a male or female default voice. Vox reporter Sigal Samuel notes, “Siri no longer responds to ‘You’re a b***h’ with ‘I’d blush if I could’ — she now says, ‘I don’t know how to respond to that’” (Samuel, 2019).  Alexa responds to similar comments by saying “I’m not sure what outcome you expected” (Samuel, 2019). 
The efforts of VA critics have paid off, but there is even more room for growth with these responses. Quartz reporter Leah Fessler asks, “What if instead of ‘I don’t think I can help you with that’ as a response to ‘Can I f*** you?’ Cortana said ‘Absolutely not, and your language sounds like sexual harassment. Here’s a link that will explain how to respectfully ask for consent’ (Fessler, 2017). Tech companies should not simply avoid perpetuating sexism — they should play an active role in uprooting beliefs about female subservience. 
Public pressure alone may not be enough of a motivator for big tech companies to reach this level of allyship. However, the threat of competition may be. Brand new technologies are now emerging to replace our current tools. Q, unveiled in 2019, is the first completely gender-neutral VA. As developer Julie Carpenter expressed, the goal of Q is to spark a global discussion about gender, technology, and ethics (Samuel, 2019). A nonprofit called Feminist Internet released their Personal Intelligent Assistant Standards and Feminist Chatbot Design Process, both of which provide guidelines for designers who want to create more equitable VAs (Samuel, 2019). They also came out with a feminist chatbot called F’xa intended to educate users about their biases. As F’xa itself puts it, “technology companies have a responsibility to challenge these kinds of market preferences, not just blindly follow them” (Samuel, 2019). I would urge both engineers in the VA industry and users of VA technologies to investigate these ethical alternatives.
In recent years, we have also seen a push to make the field of AI much more diverse. An AI Now study revealed that women comprise around 15% of AI researchers at Facebook and 10% at Google (West et. al., 2019). Nonbinary and transgender individuals are also underrepresented. Less than 5% of the staff at Facebook, Google and Microsoft are Black, and less than 6% are Hispanic (West et. al., 2019). Funding programs focused on inclusivity, reporting diversity statistics transparently, and increasing visibility for women and minorities already in the field can help to close this gap. Samuel writes, “it’s difficult to imagine a tech team composed mostly of women putting out a product that responds to “Who’s your daddy?” with ‘You are’ — which used to be Siri’s response”  (Samuel, 2019). If marginalized groups are not part of the conversation around voice assistants, it is unlikely that we will ever see improvement. 